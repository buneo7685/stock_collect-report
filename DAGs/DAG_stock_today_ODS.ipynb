{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "from airflow import DAG\n",
    "from airflow.models import Variable\n",
    "from airflow.operators.python import PythonOperator , BranchPythonOperator\n",
    "from airflow.operators.dummy import DummyOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "import pandas\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import slack_sdk\n",
    "from slack_sdk.errors import SlackApiError\n",
    "\n",
    "import psycopg2\n",
    "plt.rcParams['font.sans-serif'] = ['Taipei Sans TC Beta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 9, 17, 9, 30)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date = Variable.get('DAG_STARTDATE')\n",
    "start_date = datetime.datetime.strptime(start_date + ' 09:30:00' , '%Y%m%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'owner' : 'Buneo' ,\n",
    "    'depends_on_past' : False ,\n",
    "    'start_date' : start_date ,\n",
    "    'email_on_failure' : 'buneostock@gmail.com' ,\n",
    "    'email_on_retry' : 'buneostock@gmail.com' ,\n",
    "    'retreis' : 2 ,\n",
    "    'retry_delay' : datetime.timedelta(minutes = 5)\n",
    "        }\n",
    "\n",
    "dag = DAG('stock_today_ODS',\n",
    "          description = '' ,\n",
    "          schedule_interval = '30 9 * * 1-5',\n",
    "          default_args = args ,\n",
    "          tags = ['stock'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_password = Variable.get(\"Postgres_password\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataBase Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(host = '127.0.0.1' , dbname = 'postgres' , user = 'postgres' , password = postgres_password)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  CHECK_WORK_DATE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 Function 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CHECK():\n",
    "    lastest_workdate = pandas.read_sql(\"select date from work_date order by date desc\", con = conn)['date'].iloc[0]\n",
    "    lastest_issued_amount_date = pandas.read_sql(\"select date from issued_amounts order by date desc\", con = conn)['date'].iloc[0]\n",
    "    \n",
    "    if lastest_workdate == lastest_issued_amount_date:\n",
    "        if datetime.date.today() == lastest_workdate :\n",
    "            return ['task_INSTI_INVESTOR_SUMMARIZE' , 'task_STOCK_SMA' , 'task_OTC_SMA' , 'task_FI_OBS_SUM' , 'task_IT_OBS_SUM' , 'task_DL_OBS_SUM' , 'task_MACD' , 'task_BIAS' , 'task_KD' , 'task_CCT20','task_FI_EXPONENT_ENERGY']\n",
    "        else :\n",
    "            return 'task_NOT_TRADED_DATE'\n",
    "    else :\n",
    "        return 'task_ERROR_TODAY_DATA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 BranchPythonOperator 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CHECK' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-f37fa382b839>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m task_CHECK_WORK_DATE = BranchPythonOperator(task_id = 'task_CHECK_WORK_DATE',\n\u001b[0;32m----> 2\u001b[0;31m                                             \u001b[0mpython_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCHECK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m                                             dag = dag)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CHECK' is not defined"
     ]
    }
   ],
   "source": [
    "task_CHECK_WORK_DATE = BranchPythonOperator(task_id = 'task_CHECK_WORK_DATE',\n",
    "                                            python_callable = CHECK,\n",
    "                                            dag = dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERROR_TODAY_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_ERROR_TODAY_DATA = DummyOperator(task_id = 'task_ERROR_TODAY_DATA' , \n",
    "                                     dag = dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOT_TRADED_DATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 DummpyOperator 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_NOT_TRADED_DATE = DummyOperator(task_id = 'task_NOT_TRADED_DATE' , \n",
    "                                     dag = dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSTI_INVESTOR_SUMMARIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 Function 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def INSTI_INVESTOR_SUMMARIZE():\n",
    "    \n",
    "    work_D = [pandas.read_sql(\"select date from work_date order by date desc\" , con = conn)['date'].iloc[0]]\n",
    "    \n",
    "    for D in work_D:\n",
    "        \n",
    "        process_day = '{:0>4}-{:0>2}-{:0>2}'.format(D.year , D.month , D.day)\n",
    "        \n",
    "        cur.execute(\"delete from ods.institutional_investor where date = '{}'\".format(process_day))\n",
    "        cur.execute(\"commit\")\n",
    "        \n",
    "        ii = pandas.read_sql(\"select main.date , main.no , main.quantity , main.type , d.date_no from \\\n",
    "                         (select date , lpad(no,4,'0') as no , quantity , 'Foreign Investor' as type from foreign_investor where date = '{0}'\\\n",
    "                          union\\\n",
    "                          select date , lpad(no,4,'0') as no , quantity , 'Investment Trust' as type from investment_trust where date = '{0}'\\\n",
    "                          union\\\n",
    "                          select date , lpad(no,4,'0') as no , quantity , 'Dealer' as type from dealer where date = '{0}' ) main join work_date d on main.date = d.date\".format(process_day), con = conn)\n",
    "        \n",
    "        for _ ,data in ii.iterrows():\n",
    "            \n",
    "            cur.execute(\"insert into ods.institutional_investor (date ,no , quantity , type , date_no) values (%s,%s,%s,%s,%s)\",data)\n",
    "        cur.execute('commit')\n",
    "            \n",
    "        print('【 Institional Investor 】{} data inserted .'.format(process_day))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 PythonOperator 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_INSTI_INVESTOR_SUMMARIZE = PythonOperator(task_id = 'task_INSTI_INVESTOR_SUMMARIZE' ,\n",
    "                                               python_callable = INSTI_INVESTOR_SUMMARIZE ,\n",
    "                                               dag = dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOCK_SMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 Function 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def STOCK_SMA():\n",
    "    \n",
    "    insert_D = [pandas.read_sql(\"select date from work_date order by date desc\" , con = conn)['date'].iloc[0]]\n",
    "    \n",
    "    for iD in insert_D:\n",
    "        \n",
    "        # 讀取完整STOCK完整DAILY資料\n",
    "        main = pandas.read_sql(\"select main.*, avg.price as avg_p from (select stock.no , stock.name , stock.close , stock.quantity , stock.date , wk.date_no                              from work_date wk                              left join stock_daily stock                                on wk.date = stock.date ) main                      join stock_daily_avg_price avg on main.date = avg.date and main.no = avg.no \", con = conn)\n",
    "        \n",
    "        # 將欄位 no , name , date 處理成category格式減少記憶體負擔    \n",
    "        main[['no','name']] = main[['no','name']].astype('category')\n",
    "        main['date'] = main['date'].astype('string')\n",
    "        \n",
    "        # 新增每日總量欄位 = 均價 × 張數 \n",
    "        main['amount'] = main.avg_p * main.quantity\n",
    "        main.sort_values(['no','date_no'], inplace = True)\n",
    "        \n",
    "        # 新增前59天JOIN KEY\n",
    "        for i in range(1,60):\n",
    "            main['key_{}'.format(i)] = main['date_no'] - i\n",
    "            main['key_{}'.format(i)] = main['key_{}'.format(i)].astype('category')\n",
    "        \n",
    "        # tmp為LEFT JOIN的表格 ，只留需計算的欄位\n",
    "        tmp = main[['no','date_no','amount','quantity','close']]\n",
    "        \n",
    "        # 取得處理日期的DATE_NO , 並留下需insert的日期資料\n",
    "        iD = '{:0>4}-{:0>2}-{:0>2}'.format(iD.year,iD.month,iD.day)\n",
    "        ino = pandas.read_sql(\"select distinct date_no , date from work_date where date = '{}'\".format(iD) , con = conn)['date_no'][0]\n",
    "        main = main[main['date_no'] == ino]\n",
    "        \n",
    "        print('Start : \\033[1mMERGE\\033[0m data within 60 days')\n",
    "        \n",
    "        # 將60天內的資料合併成一列 , 並刪除join key\n",
    "        for i in range(1,60):\n",
    "            main = pandas.merge(main , tmp , left_on = ['no','key_{}'.format(i)] , right_on = ['no','date_no'] , suffixes = ('','_y_{}'.format(i)))\n",
    "            main.drop(['date_no_y_{}'.format(i),'key_{}'.format(i)], axis = 1 , inplace = True)\n",
    "    \n",
    "        print('Start : \\033[1mAGGREGATE\\033[0m close & average price')   \n",
    "    \n",
    "    # 計算各頻率 SMA & AVG\n",
    "        freq = [5,10,20,60]\n",
    "        for f in freq:\n",
    "            main['sma_{}'.format(f)] = round(main.filter(like = 'close').iloc[:,:f].mean(axis =1),2)\n",
    "            main['amt_{}'.format(f)] = main.filter(like = 'amount').iloc[:,:f].sum(axis =1)\n",
    "            main['qty_{}'.format(f)] = main.filter(like = 'quantity').iloc[:,:f].sum(axis =1)\n",
    "            main['avg_{}'.format(f)] = round((main['amt_{}'.format(f)] / main['qty_{}'.format(f)]),2)\n",
    "            main.drop(['amt_{}'.format(f),'qty_{}'.format(f)] , axis = 1 , inplace = True)        \n",
    "        main.drop(main.filter(regex=r'amount\\_') , axis = 1 , inplace = True )\n",
    "        main.drop(main.filter(regex=r'close\\_') , axis = 1 , inplace = True )\n",
    "        main.drop(main.filter(regex=r'quantity\\_') , axis = 1 , inplace = True )\n",
    "        main['date_no'] = pandas.read_sql(\"select date_no from work_date where date = '{}'\".format(iD) , con = conn )['date_no'][0]\n",
    "        main = main[['date','no','name','close','avg_p','avg_5','avg_10','avg_20','avg_60','sma_5','sma_10','sma_20','sma_60','date_no']]\n",
    "        \n",
    "        tmp = pandas.read_sql(\"select no , date from ods.analyze_avg where date = '{}' \".format(iD) , con = conn)\n",
    "        \n",
    "        i_table = pandas.merge(left = main , right = tmp , how = 'left', on = 'no')\n",
    "        i_table = i_table[i_table['date_y'].isnull()].iloc[:,:-1]\n",
    "        \n",
    "        print('Start : \\033[1mINSERT\\033[0m to target table')\n",
    "        \n",
    "        if main.shape[0] == 0:\n",
    "            pass\n",
    "        else :\n",
    "            for _ , data in i_table.iterrows():\n",
    "                cur.execute(\"insert into ods.analyze_avg (date ,no ,name , close ,avg_p ,avg5 ,avg10 ,avg20 ,avg60 ,sma5 ,sma10 ,sma20 ,sma60 , date_no) values (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\" , data)\n",
    "            cur.execute('commit')\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 PythonOperator 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_STOCK_SMA = PythonOperator(task_id = 'task_STOCK_SMA' ,\n",
    "                                python_callable = STOCK_SMA , \n",
    "                                dag = dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTC_SMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 Function 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OTC_SMA():\n",
    "    \n",
    "    insert_D = [pandas.read_sql(\"select date from work_date order by date desc\" , con = conn)['date'].iloc[0]]\n",
    "\n",
    "    for iD in insert_D:\n",
    "        \n",
    "        # 讀取完整STOCK完整DAILY資料\n",
    "        main = pandas.read_sql(\"select main.*, avg.price as avg_p\\\n",
    "                     from (select stock.no , stock.name , stock.close , stock.quantity , stock.date , wk.date_no \\\n",
    "                             from work_date wk \\\n",
    "                             left join otc_daily stock \\\n",
    "                               on wk.date = stock.date ) main \\\n",
    "                     join stock_daily_avg_price avg on main.date = avg.date and main.no = avg.no \", con = conn)\n",
    "        \n",
    "        # 將欄位 no , name , date 處理成category格式減少記憶體負擔    \n",
    "        main[['no','name']] = main[['no','name']].astype('category')\n",
    "        main['date'] = main['date'].astype('string')\n",
    "        \n",
    "        # 新增每日總量欄位 = 均價 × 張數 \n",
    "        main['amount'] = main.avg_p * main.quantity\n",
    "        main.sort_values(['no','date_no'], inplace = True)\n",
    "        \n",
    "        # 新增前59天JOIN KEY\n",
    "        for i in range(1,60):\n",
    "            main['key_{}'.format(i)] = main['date_no'] - i\n",
    "            main['key_{}'.format(i)] = main['key_{}'.format(i)].astype('category')\n",
    "        \n",
    "        # tmp為LEFT JOIN的表格 ，只留需計算的欄位\n",
    "        tmp = main[['no','date_no','amount','quantity','close']]\n",
    "        \n",
    "        # 取得處理日期的DATE_NO , 並留下需insert的日期資料\n",
    "        iD = '{:0>4}-{:0>2}-{:0>2}'.format(iD.year,iD.month,iD.day)\n",
    "        ino = pandas.read_sql(\"select distinct date_no , date from work_date where date = '{}'\".format(iD) , con = conn)['date_no'][0]\n",
    "        main = main[main['date_no'] == ino]\n",
    "        \n",
    "        print('Start : \\033[1mMERGE\\033[0m data within 60 days')\n",
    "        \n",
    "        # 將60天內的資料合併成一列 , 並刪除join key\n",
    "        for i in range(1,60):\n",
    "            main = pandas.merge(main , tmp , left_on = ['no','key_{}'.format(i)] , right_on = ['no','date_no'] , suffixes = ('','_y_{}'.format(i)))\n",
    "            main.drop(['date_no_y_{}'.format(i),'key_{}'.format(i)], axis = 1 , inplace = True)\n",
    "        \n",
    "        print('Start : \\033[1mAGGREGATE\\033[0m close & average price')   \n",
    "        \n",
    "        # 計算各頻率 SMA & AVG\n",
    "        freq = [5,10,20,60]\n",
    "        for f in freq:\n",
    "            main['sma_{}'.format(f)] = round(main.filter(like = 'close').iloc[:,:f].mean(axis =1),2)\n",
    "            main['amt_{}'.format(f)] = main.filter(like = 'amount').iloc[:,:f].sum(axis =1)\n",
    "            main['qty_{}'.format(f)] = main.filter(like = 'quantity').iloc[:,:f].sum(axis =1)\n",
    "            main['avg_{}'.format(f)] = round((main['amt_{}'.format(f)] / main['qty_{}'.format(f)]),2)\n",
    "            main.drop(['amt_{}'.format(f),'qty_{}'.format(f)] , axis = 1 , inplace = True)\n",
    "            \n",
    "        # 刪除不需要INSERT的欄位\n",
    "        main.drop(main.filter(regex=r'amount\\_') , axis = 1 , inplace = True )\n",
    "        main.drop(main.filter(regex=r'close\\_') , axis = 1 , inplace = True )\n",
    "        main.drop(main.filter(regex=r'quantity\\_') , axis = 1 , inplace = True )\n",
    "        main['date_no'] = pandas.read_sql(\"select date_no from work_date where date = '{}'\".format(iD) , con = conn )['date_no'][0]\n",
    "        main = main[['date','no','name','close','avg_p','avg_5','avg_10','avg_20','avg_60','sma_5','sma_10','sma_20','sma_60','date_no']]\n",
    "        \n",
    "        tmp = pandas.read_sql(\"select no , date from ods.analyze_avg where date = '{}' \".format(iD) , con = conn)\n",
    "        \n",
    "        i_table = pandas.merge(left = main , right = tmp , how = 'left', on = 'no')\n",
    "        i_table = i_table[i_table['date_y'].isnull()].iloc[:,:-1]\n",
    "        \n",
    "        \n",
    "        print('Start : \\033[1mINSERT\\033[0m to target table')\n",
    "        \n",
    "        if main.shape[0] == 0:\n",
    "            pass\n",
    "        else :\n",
    "            for _ , data in i_table.iterrows():\n",
    "                cur.execute(\"insert into ods.analyze_avg (date ,no ,name , close ,avg_p ,avg5 ,avg10 ,avg20 ,avg60 ,sma5 ,sma10 ,sma20 ,sma60 , date_no) \\\n",
    "                values (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\" , data)\n",
    "        \n",
    "            cur.execute('commit')\n",
    "        print('')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 PythonOperator 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_OTC_SMA = PythonOperator(task_id = 'task_OTC_SMA' ,\n",
    "                                python_callable = OTC_SMA , \n",
    "                                dag = dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FI_OBS_SUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 Function 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FI_OBS_SUM():\n",
    "    \n",
    "    insert_D = [pandas.read_sql(\"select date from work_date order by date desc\" , con = conn)['date'].iloc[0]]\n",
    "\n",
    "    for iD in insert_D:\n",
    "\n",
    "        #iD=datetime.datetime.today()\n",
    "        D = '{:0>4}-{:0>2}-{:0>2}'.format(iD.year,iD.month,iD.day)\n",
    "        \n",
    "        # 取得所有股票代號 , 並分為五等份執行\n",
    "        stock_no = pandas.read_sql(\"select no from stock_daily union select no from otc_daily\",con = conn)\n",
    "        step = int((len(stock_no)/5)+0.5)\n",
    "        \n",
    "        # 初始化批次區間變數\n",
    "        x = 0\n",
    "        y = step\n",
    "        \n",
    "        for part in range(5):      \n",
    "            \n",
    "            process_stock = tuple(stock_no.iloc[x:y,0].values)\n",
    "            fi = pandas.read_sql(\"select distinct head.no , head.date , head.date_no , main.quantity \\\n",
    "                            from (select stock.* , wk.date_no \\\n",
    "                                    from (select no , date from stock_daily a where no in {0} union  select no , date from otc_daily b where no in {0}) stock \\\n",
    "                                    join work_date wk \\\n",
    "                                      on stock.date = wk.date) head \\\n",
    "                            left join (select distinct *  from foreign_investor ) main \\\n",
    "                              on head.no = main.no \\\n",
    "                             and head.date = main.date\".format(process_stock) , con = conn).fillna(0)\n",
    "            \n",
    "            fi_tmp = fi[['no','date_no','quantity']]\n",
    "            fi = fi[fi.date.astype('string') == D ]\n",
    "        \n",
    "            \n",
    "            for i in range(1,60):\n",
    "                fi['key_{}'.format(i)] = fi.date_no - i\n",
    "            \n",
    "            for i in range(1,60):\n",
    "                fi = pandas.merge(fi,fi_tmp , left_on = ['no','key_{}'.format(i)], right_on =['no','date_no'] , suffixes = ('','_y{}'.format(i)))\n",
    "        \n",
    "                fi.drop(['key_{}'.format(i),'date_no_y{}'.format(i)], axis = 1 , inplace = True)\n",
    "          \n",
    "            fi['fi_sum5'] = fi.filter(like='quantity').iloc[:,:5].sum(axis = 1)\n",
    "            fi['fi_sum10'] = fi.filter(like='quantity').iloc[:,:10].sum(axis = 1)\n",
    "            fi['fi_sum20'] = fi.filter(like='quantity').iloc[:,:20].sum(axis = 1)\n",
    "            fi['fi_sum60'] = fi.filter(like='quantity').iloc[:,:60].sum(axis = 1)\n",
    "            fi.drop(fi.filter(regex = 'quantity\\_').columns , axis =1 , inplace = True)\n",
    "            fi['date_no'] = pandas.read_sql(\"select date_no from work_date where date = '{}'\".format(D) , con = conn )['date_no'][0]\n",
    "            fi = fi[['no','date','quantity','fi_sum5','fi_sum10','fi_sum20','fi_sum60','date_no']]\n",
    "\n",
    "            tmp = pandas.read_sql(\"select no , date from ods.analyze_FI_OBS where date = '{}' \".format(D) , con = conn)\n",
    "        \n",
    "            i_table = pandas.merge(left = fi , right = tmp , how = 'left', on = 'no')\n",
    "            i_table = i_table[i_table['date_y'].isnull()].iloc[:,:-1]            \n",
    "            \n",
    "            for _ , data in i_table.iterrows():\n",
    "                cur.execute('insert into ods.analyze_FI_OBS (no , date , quantity,sum5,sum10,sum20,sum60,date_no) \\\n",
    "            values (%s,%s,%s,%s,%s,%s,%s,%s)' , data)\n",
    "            \n",
    "            cur.execute('commit')\n",
    "            print('【 Tech Analyze Foreign Investor Over Bought / Sold 】{0} data inserted {1}/5 .'.format(D,part+1))\n",
    "            x = y\n",
    "            y += step\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 PythonOperator 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_FI_OBS_SUM = PythonOperator(task_id = 'task_FI_OBS_SUM' ,\n",
    "                                 python_callable = FI_OBS_SUM ,\n",
    "                                 dag = dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IT_OBS_SUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 Function 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IT_OBS_SUM():\n",
    "    \n",
    "    insert_D = [pandas.read_sql(\"select date from work_date order by date desc\" , con = conn)['date'].iloc[0]]\n",
    "\n",
    "    for iD in insert_D:\n",
    "    \n",
    "        #iD=datetime.datetime.today()\n",
    "        D = '{:0>4}-{:0>2}-{:0>2}'.format(iD.year,iD.month,iD.day)\n",
    "        \n",
    "        # 取得所有股票代號 , 並分為五等份執行\n",
    "        stock_no = pandas.read_sql(\"select no from stock_daily union select no from otc_daily\",con = conn)\n",
    "        step = int((len(stock_no)/5)+0.5)\n",
    "        \n",
    "        # 初始化批次區間變數\n",
    "        x = 0\n",
    "        y = step\n",
    "        \n",
    "        for part in range(5):      \n",
    "            \n",
    "            process_stock = tuple(stock_no.iloc[x:y,0].values)\n",
    "            fi = pandas.read_sql(\"select distinct head.no , head.date , head.date_no , main.quantity \\\n",
    "                                from (select stock.* , wk.date_no \\\n",
    "                                        from (select no , date from stock_daily a where no in {0} union  select no , date from otc_daily b where no in {0}) stock \\\n",
    "                                        join work_date wk \\\n",
    "                                      on stock.date = wk.date) head \\\n",
    "                            left join (select distinct * from investment_trust) main \\\n",
    "                              on head.no = main.no \\\n",
    "                             and head.date = main.date\".format(process_stock) , con = conn).fillna(0)\n",
    "            \n",
    "            fi_tmp = fi[['no','date_no','quantity']]\n",
    "            \n",
    "            fi = fi[fi.date.astype('string') == D ]\n",
    "            \n",
    "            \n",
    "            for i in range(1,60):\n",
    "                fi['key_{}'.format(i)] = fi.date_no - i\n",
    "                \n",
    "            for i in range(1,60):\n",
    "                fi = pandas.merge(fi,fi_tmp , left_on = ['no','key_{}'.format(i)], right_on =['no','date_no'] , suffixes = ('','_y{}'.format(i)))\n",
    "                \n",
    "                fi.drop(['key_{}'.format(i),'date_no_y{}'.format(i)], axis = 1 , inplace = True)\n",
    "            \n",
    "            fi['it_sum5'] = fi.filter(like='quantity').iloc[:,:5].sum(axis = 1)\n",
    "            fi['it_sum10'] = fi.filter(like='quantity').iloc[:,:10].sum(axis = 1)\n",
    "            fi['it_sum20'] = fi.filter(like='quantity').iloc[:,:20].sum(axis = 1)\n",
    "            fi['it_sum60'] = fi.filter(like='quantity').iloc[:,:60].sum(axis = 1)\n",
    "            fi.drop(fi.filter(regex = 'quantity\\_').columns , axis =1 , inplace = True)\n",
    "            fi['date_no'] = pandas.read_sql(\"select date_no from work_date where date = '{}'\".format(D) , con = conn )['date_no'][0]\n",
    "            fi = fi[['no','date','quantity','it_sum5','it_sum10','it_sum20','it_sum60','date_no']]\n",
    "        \n",
    "            tmp = pandas.read_sql(\"select no , date from ods.analyze_IT_OBS where date = '{}' \".format(D) , con = conn)\n",
    "        \n",
    "            i_table = pandas.merge(left = fi , right = tmp , how = 'left', on = 'no')\n",
    "            i_table = i_table[i_table['date_y'].isnull()].iloc[:,:-1]  \n",
    "            \n",
    "            for _ , data in i_table.iterrows():\n",
    "                cur.execute('insert into ods.analyze_IT_OBS (no , date , quantity,sum5,sum10,sum20,sum60,date_no) \\\n",
    "            values (%s,%s,%s,%s,%s,%s,%s,%s)' , data)\n",
    "            \n",
    "            cur.execute('commit')\n",
    "            print('【 Tech Analyze Investment Trust Over Bought / Sold 】{0} data inserted {1}/5 .'.format(D,part+1))\n",
    "            x = y\n",
    "            y += step\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 PythonOperator 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_IT_OBS_SUM = PythonOperator(task_id = 'task_IT_OBS_SUM' ,\n",
    "                                 python_callable = IT_OBS_SUM ,\n",
    "                                 dag = dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL_OBS_SUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 Function 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DL_OBS_SUM():\n",
    "    \n",
    "    insert_D = [pandas.read_sql(\"select date from work_date order by date desc\" , con = conn)['date'].iloc[0]]\n",
    "\n",
    "    for iD in insert_D:\n",
    "    \n",
    "        #iD=datetime.datetime.today()\n",
    "        D = '{:0>4}-{:0>2}-{:0>2}'.format(iD.year,iD.month,iD.day)\n",
    "        \n",
    "        # 取得所有股票代號 , 並分為五等份執行\n",
    "        stock_no = pandas.read_sql(\"select no from stock_daily union select no from otc_daily\",con = conn)\n",
    "        step = int((len(stock_no)/5)+0.5)\n",
    "        \n",
    "        # 初始化批次區間變數\n",
    "        x = 0\n",
    "        y = step\n",
    "        \n",
    "        for part in range(5):      \n",
    "            \n",
    "            process_stock = tuple(stock_no.iloc[x:y,0].values)\n",
    "            dl = pandas.read_sql(\"select distinct head.no , head.date , head.date_no , main.quantity \\\n",
    "                            from (select stock.* , wk.date_no \\\n",
    "                                    from (select no , date from stock_daily a where no in {0} union  select no , date from otc_daily b where no in {0}) stock \\\n",
    "                                    join work_date wk \\\n",
    "                                      on stock.date = wk.date) head \\\n",
    "                            left join (select distinct *  from dealer ) main \\\n",
    "                              on head.no = main.no \\\n",
    "                             and head.date = main.date\".format(process_stock) , con = conn).fillna(0)\n",
    "            \n",
    "            dl_tmp = dl[['no','date_no','quantity']]\n",
    "            dl = dl[dl.date.astype('string') == D ]\n",
    "    \n",
    "        \n",
    "            for i in range(1,60):\n",
    "                dl['key_{}'.format(i)] = dl.date_no - i\n",
    "            \n",
    "            for i in range(1,60):\n",
    "                dl = pandas.merge(dl,dl_tmp , left_on = ['no','key_{}'.format(i)], right_on =['no','date_no'] , suffixes = ('','_y{}'.format(i)))\n",
    "        \n",
    "                dl.drop(['key_{}'.format(i),'date_no_y{}'.format(i)], axis = 1 , inplace = True)\n",
    "          \n",
    "            dl['dl_sum5'] = dl.filter(like='quantity').iloc[:,:5].sum(axis = 1)\n",
    "            dl['dl_sum10'] = dl.filter(like='quantity').iloc[:,:10].sum(axis = 1)\n",
    "            dl['dl_sum20'] = dl.filter(like='quantity').iloc[:,:20].sum(axis = 1)\n",
    "            dl['dl_sum60'] = dl.filter(like='quantity').iloc[:,:60].sum(axis = 1)\n",
    "            dl.drop(dl.filter(regex = 'quantity\\_').columns , axis =1 , inplace = True)\n",
    "            dl['date_no'] = pandas.read_sql(\"select date_no from work_date where date = '{}'\".format(D) , con = conn )['date_no'][0]\n",
    "            dl = dl[['no','date','quantity','dl_sum5','dl_sum10','dl_sum20','dl_sum60','date_no']]\n",
    "            \n",
    "            tmp = pandas.read_sql(\"select no , date from ods.analyze_DL_OBS where date = '{}' \".format(D) , con = conn)\n",
    "        \n",
    "            i_table = pandas.merge(left = dl , right = tmp , how = 'left', on = 'no')\n",
    "            i_table = i_table[i_table['date_y'].isnull()].iloc[:,:-1]         \n",
    "            \n",
    "            for _ , data in i_table.iterrows():\n",
    "                cur.execute('insert into ods.analyze_DL_OBS (no , date , quantity,sum5,sum10,sum20,sum60,date_no) \\\n",
    "            values (%s,%s,%s,%s,%s,%s,%s,%s)' , data)\n",
    "            \n",
    "            cur.execute('commit')\n",
    "            print('【 Tech Analyze Dealer Over Bought / Sold 】{0} data inserted {1}/5 .'.format(D,part+1))\n",
    "            x = y\n",
    "            y += step\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 PythonOperator 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_DL_OBS_SUM = PythonOperator(task_id = 'task_DL_OBS_SUM' ,\n",
    "                                 python_callable = DL_OBS_SUM ,\n",
    "                                 dag = dag )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 Function 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MACD():\n",
    "    n = 12\n",
    "    m = 26\n",
    "    x = 9\n",
    "    \n",
    "    work_D = [pandas.read_sql(\"select date from work_date order by date desc\" , con = conn)['date'].iloc[0]]\n",
    "    \n",
    "    for insert_D in work_D:\n",
    "\n",
    "        D = '{:0>4}-{:0>2}-{:0>2}'.format(insert_D.year,insert_D.month,insert_D.day)\n",
    "        all_stock = pandas.read_sql(\"select no from stock_daily where date = '{0}' union select no from otc_daily where date = '{0}'\".format(D) , con = conn)['no']\n",
    "        \n",
    "        for i_stock in range(len(all_stock)):\n",
    "            \n",
    "            TA = all_stock[i_stock]\n",
    "            today = pandas.read_sql(\"select m.* , d.date_no from (select date , no , close  from stock_daily where no = '{0}'  union select date , no , close  from otc_daily where no = '{0}' ) m \\\n",
    "                                                            join work_date d \\\n",
    "                                                              on m.date = d.date \\\n",
    "                                                           where d.date = '{1}' \".format(TA , D) , con = conn)\n",
    "            today[['nEMA','mEMA','DIF','MACD','BAR']] = 0.0\n",
    "            date_no = today['date_no'][0]\n",
    "            yesterday = pandas.read_sql(\"select date , date_no , no , nEMA , mEMA , MACD from ods.macd where no = '{0}' and date_no = {1}\".format(TA , date_no-1) , con = conn)\n",
    "            \n",
    "            if yesterday.shape[0] == 0:\n",
    "                today['nEMA'] = today['close'] * 2 / (n + 1)\n",
    "                today['mEMA'] = today['close'] * 2 / (m + 1)\n",
    "                today['DIF'] = today['nEMA'] - today['mEMA']\n",
    "                today['MACD'] = today['DIF'] * 2 / (x + 1)\n",
    "                today['BAR'] = today['DIF'] - today['MACD']\n",
    "        \n",
    "            else:\n",
    "                today['nEMA'] = ((yesterday['nema'][0] * (n - 1)) + (today['close'] * 2)) / (n + 1)\n",
    "                today['mEMA'] = ((yesterday['mema'][0] * (m - 1)) + (today['close'] * 2)) / (m + 1)\n",
    "                today['DIF'] = today['nEMA'] - today['mEMA']\n",
    "                today['MACD'] = ((yesterday['macd'][0] * (x - 1)) + today['DIF'][0] * 2) / (x + 1)\n",
    "                today['BAR'] = today['DIF'] - today['MACD']\n",
    "            today[['BAR','nEMA','mEMA','MACD']] = round(today[['BAR','nEMA','mEMA','MACD']],3)     \n",
    "            today = today[['date','date_no','no','BAR','nEMA','mEMA','MACD']]\n",
    "            \n",
    "            tmp = pandas.read_sql(\"select no , date from ods.macd where date = '{}' \".format(D) , con = conn)\n",
    "        \n",
    "            i_table = pandas.merge(left = today , right = tmp , how = 'left', on = 'no')\n",
    "            i_table = i_table[i_table['date_y'].isnull()].iloc[:,:-1]\n",
    "        \n",
    "            for _,data in i_table.iterrows():\n",
    "                \n",
    "                cur.execute(\"insert into ods.macd (date , date_no , no , macd_bar , nema , mema , macd) values (%s,%s,%s,%s,%s,%s,%s)\",data)\n",
    "            cur.execute(\"commit\")\n",
    "            \n",
    "        print(\"【 MACD 】{} data inserted. \".format(insert_D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 PythonOperator 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_MACD = PythonOperator(task_id = 'task_MACD' ,\n",
    "                           python_callable = MACD ,\n",
    "                           dag = dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 Function 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KD():\n",
    "    \n",
    "    work_D = [pandas.read_sql(\"select date from work_date order by date desc\" , con = conn)['date'].iloc[0]]\n",
    "    \n",
    "    for D in work_D:\n",
    "        insert_D = '{:0>4}-{:0>2}-{:0>2}'.format(D.year,D.month,D.day)\n",
    "        all_list = pandas.read_sql(\"select main.no from (select date,no from stock_daily where date = '{0}'  union \\\n",
    "                         select date , no from otc_daily where date = '{0}' ) main left join ods.kd k on main.date =k.date and main.no = k.no where k.no is null\".format(insert_D) , con = conn)['no'].tolist()\n",
    "        \n",
    "        date_list = tuple(pandas.read_sql(\"select cast(date as varchar) from work_date where date_no >= (select date_no from work_date where date = '{0}') -8 \\\n",
    "                                              and date_no <= (select date_no from work_date where date = '{0}')\".format(insert_D) , con = conn)['date'].tolist())\n",
    "        try :\n",
    "            date_no = str(pandas.read_sql(\"select date_no from work_date where date = '{0}'\".format(insert_D) , con = conn)['date_no'][0])\n",
    "        except IndexError:\n",
    "            pass\n",
    "        \n",
    "        for no in range(len(all_list)):\n",
    "            kd_ini = pandas.read_sql(\"select * from ods.kd where no = '{0}'\".format(all_list[no]) , con = conn)\n",
    "            if len(kd_ini) < 8 :           \n",
    "                cur.execute(\"insert into ods.kd (date ,date_no, no , k_value , d_value) values (%s,%s,%s,%s,%s)\" , (insert_D ,date_no , all_list[no] , 50,50))\n",
    "                \n",
    "            else :\n",
    "                \n",
    "                process_no = pandas.read_sql(\"select * from (select date , no , close , highest , lowest from stock_daily where no = '{0}' and date in {1} union \\\n",
    "                                                             select date , no , close , highest , lowest from   otc_daily where no = '{0}' and date in {1}) main \\\n",
    "                                                      order by date \".format(all_list[no],date_list) , con = conn)\n",
    "                #display(process_no)\n",
    "                min_9 = process_no[process_no['lowest']>0]['lowest'].min()\n",
    "                max_9 = process_no[process_no['highest']>0]['highest'].max()\n",
    "                if min_9 == max_9:\n",
    "                    RSV = 0\n",
    "                else :\n",
    "                    #display(process_no[process_no['close']>0])\n",
    "                    #display(all_list[no])\n",
    "                    try:\n",
    "                        RSV = ((process_no[process_no['close']>0]['close'].iloc[-1]-min_9) / (max_9 - min_9))*100\n",
    "                        last_kd = pandas.read_sql(\"select k_value , d_value from ods.kd where no = '{0}' order by date desc limit 1\".format(all_list[no],date_list[-2]) , con = conn)\n",
    "                        last_k = last_kd['k_value'][0]\n",
    "                        last_d = last_kd['d_value'][0]            \n",
    "                        \n",
    "                        today_k = (last_k/3)*2 + (RSV/3)\n",
    "                        today_d = (last_d/3)*2 + (today_k/3)\n",
    "                        \n",
    "                        #print(RSV,today_k , today_d)\n",
    "                \n",
    "                        cur.execute(\"insert into ods.kd (date , date_no , no , k_value , d_value) values (%s,%s,%s,%s,%s)\" , (insert_D , date_no , all_list[no] , today_k , today_d))\n",
    "                        cur.execute(\"commit\")        \n",
    "        \n",
    "                    except IndexError:\n",
    "                        pass\n",
    "        if len(all_list) == 0:\n",
    "            print(\"【 KD 】{0} no need to inserted .\".format(insert_D))\n",
    "        else :\n",
    "            print(\"【 KD 】{0} data inserted .\".format(insert_D))\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 PythonOperator 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_KD = PythonOperator(task_id = 'task_KD' ,\n",
    "                           python_callable = KD ,\n",
    "                           dag = dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 Function 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BIAS():\n",
    "    \n",
    "    work_D = [pandas.read_sql(\"select date from work_date order by date desc\" , con = conn)['date'].iloc[0]]\n",
    "    \n",
    "    for D in work_D:\n",
    "        \n",
    "        insert_D = '{:0>4}-{:0>2}-{:0>2}'.format(D.year,D.month,D.day)\n",
    "        \n",
    "        all_list = pandas.read_sql(\"select main.no \\\n",
    "                                      from (select date,no from stock_daily where date = '{0}'  union \\\n",
    "                                            select date , no from otc_daily where date = '{0}' ) main \\\n",
    "                                              join ods.analyze_avg avg \\\n",
    "                                                on avg.no = main.no \\\n",
    "                                               and avg.date = main.date \\\n",
    "                                              left join ods.bias b \\\n",
    "                                                on main.date = b.date \\\n",
    "                                               and main.no = b.no \\\n",
    "                                     where b.no is null\".format(insert_D) , con = conn)['no'].tolist()\n",
    "        \n",
    "        if len(all_list) == 0:\n",
    "            print(\"【 BIAS 】{0} no need to inserted .\".format(insert_D))\n",
    "        else :\n",
    "            for i in range(len(all_list)):\n",
    "                main = pandas.read_sql(\"select * from ods.analyze_avg where date = '{0}' and no = '{1}'\".format(insert_D,all_list[i]) , con = conn)\n",
    "                main['bias_5'] = round((main['close'][0] - main['sma5'][0])/main['sma5'][0],4)\n",
    "                main['bias_20'] = round((main['close'][0] - main['sma20'][0])/main['sma20'][0],4)\n",
    "                main = main[['date','date_no','no','bias_5','bias_20']]\n",
    "                \n",
    "                tmp = pandas.read_sql(\"select no , date from ods.bias where date = '{}' \".format(D) , con = conn)\n",
    "        \n",
    "                i_table = pandas.merge(left = main , right = tmp , how = 'left', on = 'no')\n",
    "                i_table = i_table[i_table['date_y'].isnull()].iloc[:,:-1]                \n",
    "                \n",
    "                for _,data in i_table.iterrows():                                 \n",
    "                    cur.execute(\"insert into ods.bias (date , date_no , no , bias_5,bias_20) values (%s,%s,%s,%s,%s)\",data)\n",
    "                cur.execute(\"commit\")\n",
    "                \n",
    "                \n",
    "            print(\"【 BIAS 】{0} inserted .\".format(insert_D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 PythonOeprator 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_BIAS = PythonOperator(task_id = 'task_BIAS' ,\n",
    "                           python_callable = BIAS , \n",
    "                           dag = dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCT5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 Function 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CCT5():\n",
    "\n",
    "    work_D = [pandas.read_sql(\"select date from work_date order by date desc\" , con = conn)['date'].iloc[0]]\n",
    "    \n",
    "    for D in work_D:\n",
    "        \n",
    "        insert_D = '{:0>4}-{:0>2}-{:0>2}'.format(D.year,D.month,D.day)\n",
    "        \n",
    "        # 取得股市中的5天內的日期\n",
    "        html_D = pandas.read_sql(\"select y.date as first_day ,t.date as last_day \\\n",
    "                                    from work_date t \\\n",
    "                                    join work_date y \\\n",
    "                                      on t.date_no - 4 = y.date_no \\\n",
    "                                   where t.date = '{0}'\".format(insert_D) , con = conn)\n",
    "        # 排除週一到週五但無股市的日期\n",
    "        if html_D.shape[0] != 0:\n",
    "            \n",
    "            first_D = str(html_D['first_day'][0])\n",
    "            last_D = str(html_D['last_day'][0])\n",
    "            \n",
    "            # 讀取處理日期中的所有股票編號\n",
    "            all_stock = pandas.read_sql(\"select main.no  from (select no from stock_daily where date = '{0}' union select no from otc_daily where date = '{0}') main left join (select no from ods.cct5 where date = '{0}') c on c.no = main.no where c.no is null and main.no != '1418'\".format(insert_D),con = conn)\n",
    "    \n",
    "            for i in range(len(all_stock)):\n",
    "                TA = all_stock['no'][i]\n",
    "                \n",
    "                # 讀取當日以前的交易量\n",
    "                QTY = pandas.read_sql(\"select * from (select date , no , quantity from stock_daily where no = '{0}' and date <= '{1}' union select date , no , quantity from otc_daily where no = '{0}' and date <= '{1}') a order by date\".format(TA,insert_D) , con = conn)\n",
    "                # 判斷此股票是否有5天的交易日\n",
    "                if len(QTY) >= 5:\n",
    "                    \n",
    "                    sum5 = QTY['quantity'].iloc[-5:].sum()\n",
    "                    #html = 'http://sod.nsc.com.tw/z/zc/zco/zco.djhtm?a={0}&e={1}&f={2}'.format(TA,first_D,last_D)\n",
    "                    html = 'https://fubon-ebrokerdj.fbs.com.tw/z/zc/zco/zco.djhtm?a={0}&e={1}&f={2}'.format(TA,first_D,last_D)\n",
    "                    main = pandas.read_html(html)[2].iloc[-3,[1,6]]\n",
    "                    \n",
    "                    # 排除20日內無主力交易\n",
    "                    if main[1] != '買超':\n",
    "                    \n",
    "                        cct5 = round((int(main[1]) - int(main[6]))/sum5,4)\n",
    "                        date_no = pandas.read_sql(\"select date_no from work_date where date = '{}'\".format(insert_D) , con = conn )['date_no'][0]\n",
    "                        insert_data = pandas.DataFrame([[insert_D,TA,cct5,date_no]],columns = ['date','no','cct5','date_no'])\n",
    "                        \n",
    "                        for _ , data in insert_data.iterrows():\n",
    "                            try:\n",
    "                                cur.execute(\"insert into ods.cct5 (date , no , cct5,date_no) values (%s,%s,%s,%s)\",data)\n",
    "                            except :\n",
    "                                cur.execute(\"insert into ods.cct5 (date , no , cct5,date_no) values (%s,%s,%s,%s)\",(insert_D,TA,99.9999,date_no))\n",
    "                        \n",
    "                        cur.execute(\"commit\")\n",
    "                        \n",
    "                    else:\n",
    "                        pass\n",
    "                \n",
    "                else :\n",
    "                    pass\n",
    "                \n",
    "            print(\"【 CCT5 】{} data inserted. \".format(insert_D))\n",
    "            \n",
    "        else :\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 PythonOperator 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_CCT5 = PythonOperator(task_id = 'task_CCT5' ,\n",
    "                           python_callable = CCT5 , \n",
    "                           dag = dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCT20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 Function 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CCT20():\n",
    "    \n",
    "    work_D = [pandas.read_sql(\"select date from work_date order by date desc\" , con = conn)['date'].iloc[0]]\n",
    "\n",
    "    for D in work_D:\n",
    "        \n",
    "        insert_D = '{:0>4}-{:0>2}-{:0>2}'.format(D.year,D.month,D.day)\n",
    "        \n",
    "        # 取得股市中的19天前的日期\n",
    "        html_D = pandas.read_sql(\"select y.date as first_day ,t.date as last_day \\\n",
    "                                    from work_date t \\\n",
    "                                    join work_date y \\\n",
    "                                      on t.date_no - 19 = y.date_no \\\n",
    "                                   where t.date = '{0}'\".format(insert_D) , con = conn)\n",
    "        \n",
    "        # 排除週一到週五但無股市的日期\n",
    "        if html_D.shape[0] !=0:\n",
    "            \n",
    "            first_D = str(html_D['first_day'][0])\n",
    "            last_D = str(html_D['last_day'][0])\n",
    "            \n",
    "            # 讀取處理日期中的所有股票編號\n",
    "            all_stock = pandas.read_sql(\"select main.no  from (select no from stock_daily where date = '{0}' union select no from otc_daily where date = '{0}') main left join (select no from ods.cct20 where date = '{0}') c on c.no = main.no where c.no is null and main.no != '1418'\".format(insert_D),con = conn)\n",
    "    \n",
    "            for i in range(len(all_stock)):\n",
    "            \n",
    "                TA = all_stock['no'][i]\n",
    "                \n",
    "                # 讀取當日以前的交易量\n",
    "                QTY = pandas.read_sql(\"select * from (select date , no , quantity from stock_daily where no = '{0}' and date <= '{1}' union select date , no , quantity from otc_daily where no = '{0}' and date <= '{1}') a order by date\".format(TA,insert_D) , con = conn)               \n",
    "                \n",
    "                # 判斷此股票是否有20天的交易日\n",
    "                if len(QTY) >= 20:\n",
    "                    \n",
    "                    sum20 = QTY['quantity'].iloc[-20:].sum()\n",
    "                    #html = 'http://sod.nsc.com.tw/z/zc/zco/zco.djhtm?a={0}&e={1}&f={2}'.format(TA,first_D,last_D)\n",
    "                    html = 'https://fubon-ebrokerdj.fbs.com.tw/z/zc/zco/zco.djhtm?a={0}&e={1}&f={2}'.format(TA,first_D,last_D)\n",
    "                    main = pandas.read_html(html)[2].iloc[-3,[1,6]]\n",
    "                    \n",
    "                    # 排除20日內無主力交易\n",
    "                    if main[1] != '買超':\n",
    "                    \n",
    "                        cct20 = round((int(main[1]) - int(main[6]))/sum20,4)\n",
    "                        date_no =pandas.read_sql(\"select date_no from work_date where date = '{}'\".format(insert_D) , con = conn )['date_no'][0]\n",
    "                        insert_data = pandas.DataFrame([[insert_D,TA,cct20,date_no]],columns = ['date','no','cct20','date_no'])\n",
    "                        \n",
    "                        for _ , data in insert_data.iterrows():\n",
    "                            \n",
    "                            try:\n",
    "                                cur.execute(\"insert into ods.cct20 (date , no , cct20,date_no) values (%s,%s,%s,%s)\",data)\n",
    "                            except :\n",
    "                                cur.execute(\"insert into ods.cct20 (date , no , cct20,date_no) values (%s,%s,%s,%s)\",(insert_D,TA,99.9999,date_no))\n",
    "                        cur.execute(\"commit\")\n",
    "                    \n",
    "                                \n",
    "                    else:\n",
    "                        pass\n",
    "                    \n",
    "                    \n",
    "                else :\n",
    "                    pass\n",
    "                \n",
    "            print(\"【 CCT20 】{} data inserted. \".format(insert_D))\n",
    "            \n",
    "        else :\n",
    "            pass \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 PythonOperator 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_CCT20 = PythonOperator(task_id = 'task_CCT20' ,\n",
    "                            python_callable = CCT20 ,\n",
    "                            dag = dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FI_EXPONENT_ENERGY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 Function 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FI_EXPONENT_ENERGY():\n",
    "    \n",
    "    work_D = [pandas.read_sql(\"select date from work_date order by date desc\" , con = conn)['date'].iloc[0]]\n",
    "    \n",
    "    for D in work_D:\n",
    "        \n",
    "        # 串聯classify , institutional_investor , analyze_avg 取得投信及外資進出數量及成本\n",
    "        insert_D = '{:0>4}-{:0>2}-{:0>2}'.format(D.year,D.month,D.day)\n",
    "        \n",
    "        cur.execute(\"delete from ods.exponent_insti where date = '{}' and investor = 'Foreign investor'\".format(insert_D))\n",
    "        cur.execute(\"commit\")\n",
    "        \n",
    "        FI = pandas.read_sql(\"select head.type , head.classify , head.date ,head.date_no , body.no , body.qty , body.amt \\\n",
    "                                from (select distinct date , date_no , type , classify from classify c , work_date d where d.date = '{0}') head \\\n",
    "                                left join (select main.date , d.date_no ,  main.no , c.type , c.classify , main.qty , main.amt \\\n",
    "                                        from (select fi.date , fi.no , fi.quantity as qty , fi.quantity * avg.price as AMT \\\n",
    "                                             from FOREIGN_INVESTOR fi \\\n",
    "                                             join STOCK_DAILY_AVG_PRICE avg \\\n",
    "                                               on fi.date = avg.date \\\n",
    "                                              and fi.no = avg.no ) main \\\n",
    "                                             join CLASSIFY c \\\n",
    "                                               on main.no = c.no \\\n",
    "                                             join work_date d \\\n",
    "                                               on main.date = d.date \\\n",
    "                                            where main.date = '{0}' ) body \\\n",
    "                                  on head.type = body.type \\\n",
    "                                 and head.classify = body.classify \\\n",
    "                                 and head.date = body.date \".format(insert_D) \n",
    "                        , con = conn)\n",
    "        \n",
    "        # 排除insert_D為非股市日\n",
    "        if FI.shape[0] != 0 :\n",
    "            #依照組別type, topic 加總金額及數量\n",
    "            agg = FI[['type','classify','qty','amt']].groupby(['type','classify']).sum().reset_index()\n",
    "            agg['date'] = FI['date'][0]\n",
    "            agg['date_no'] = FI['date_no'][0]\n",
    "            agg['investor'] = 'Foreign investor'\n",
    "            agg[['amt','qty']].fillna(0)\n",
    "            agg = agg[['date','date_no','type','classify','qty','amt','investor']]\n",
    "            \n",
    "            \n",
    "            for _ , data in agg.iterrows():\n",
    "                cur.execute(\"insert into ods.exponent_insti (date , date_no , type , topic , qty , amt , investor ) values (%s,%s,%s,%s,%s,%s,%s)\",data)\n",
    "            cur.execute('commit')\n",
    "            \n",
    "            print('【 Exponent Institutional 】{} Foreign Investor inserted. '.format(insert_D))\n",
    "            agg = ''\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # 取得所有 type , topic\n",
    "        \n",
    "        all_type = pandas.read_sql(\"select distinct type from classify \",con = conn)\n",
    "        \n",
    "        \n",
    "        for i_type in range(len(all_type)):   \n",
    "        \n",
    "            TYPE = all_type.iloc[i_type][0]\n",
    "           \n",
    "            all_topic = pandas.read_sql(\"select distinct classify from classify where type = '{}'\".format(TYPE) , con = conn)\n",
    "            for i_topic in range(len(all_topic)):\n",
    "                \n",
    "                TOPIC = all_topic.iloc[i_topic][0]\n",
    "                \n",
    "                main = pandas.read_sql(\"select * from ods.exponent_insti \\\n",
    "                                         where type = '{0}' \\\n",
    "                                           and topic = '{1}' \\\n",
    "                                           and investor = 'Foreign investor' \\\n",
    "                                           and date_no >= (select date_no from work_date where date = '{2}')-19 \\\n",
    "                                           and date_no <= (select date_no from work_date where date = '{2}') \\\n",
    "                                         order by date \".format(TYPE , TOPIC,insert_D) , con = conn)\n",
    "                \n",
    "                if len(main) == 20:\n",
    "                    main['qty20'].iloc[-1] = main['qty'].sum()\n",
    "                    main['amt20'].iloc[-1] = main['amt'].sum()\n",
    "                    DATE = main['date'].iloc[-1]\n",
    "                    #display(main.iloc[-1])\n",
    "                    \n",
    "                    cur.execute(\"update ods.exponent_insti set qty20 = '{0}' , amt20 = '{1}' \\\n",
    "                                  where date = '{2}' and type = '{3}' and topic = '{4}' and investor = 'Foreign investor'\".format(main['qty20'].iloc[-1],main['amt20'].iloc[-1],DATE,TYPE,TOPIC))\n",
    "                    \n",
    "                    cur.execute(\"commit\")\n",
    "                else:\n",
    "                    pass\n",
    "        print('【 Exponent Institutional 】{0} qty20 & amt20 of Foreign Investor updated.'.format(insert_D))\n",
    "                  \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 PythonOperator 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_FI_EXPONENT_ENERGY = PythonOperator(task_id = 'task_FI_EXPONENT_ENERGY' ,\n",
    "                                         python_callable = FI_EXPONENT_ENERGY ,\n",
    "                                         dag = dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IT_EXPONENT_ENERGY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 Function 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IT_EXPONENT_ENERGY():\n",
    "    \n",
    "    work_D = [pandas.read_sql(\"select date from work_date order by date desc\" , con = conn)['date'].iloc[0]]\n",
    "    \n",
    "    for D in work_D:\n",
    "        \n",
    "        # 串聯classify , institutional_investor , analyze_avg 取得投信及外資進出數量及成本\n",
    "        insert_D = '{:0>4}-{:0>2}-{:0>2}'.format(D.year,D.month,D.day)\n",
    "        \n",
    "        cur.execute(\"delete from ods.exponent_insti where date = '{}' and investor = 'Investment trust'\".format(insert_D))\n",
    "        cur.execute(\"commit\")\n",
    "        \n",
    "        IT = pandas.read_sql(\"select head.type , head.classify , head.date ,head.date_no , body.no , body.qty , body.amt \\\n",
    "                                from (select distinct date , date_no , type , classify from classify c , work_date d where d.date = '{0}') head \\\n",
    "                                left join (select main.date , d.date_no ,  main.no , c.type , c.classify , main.qty , main.amt \\\n",
    "                                        from (select it.date , it.no , it.quantity as qty , it.quantity * avg.price as AMT \\\n",
    "                                             from INVESTMENT_TRUST it \\\n",
    "                                             join STOCK_DAILY_AVG_PRICE avg \\\n",
    "                                               on it.date = avg.date \\\n",
    "                                              and it.no = avg.no ) main \\\n",
    "                                             join CLASSIFY c \\\n",
    "                                               on main.no = c.no \\\n",
    "                                             join work_date d \\\n",
    "                                               on main.date = d.date \\\n",
    "                                            where main.date = '{0}' ) body \\\n",
    "                                  on head.type = body.type \\\n",
    "                                 and head.classify = body.classify \\\n",
    "                                 and head.date = body.date \".format(insert_D) \n",
    "                        , con = conn)\n",
    "        \n",
    "        # 排除insert_D為非股市日\n",
    "        if IT.shape[0] != 0 :\n",
    "            #依照組別type, topic 加總金額及數量\n",
    "            agg = IT[['type','classify','qty','amt']].groupby(['type','classify']).sum().reset_index()\n",
    "            agg['date'] = IT['date'][0]\n",
    "            agg['date_no'] = IT['date_no'][0]\n",
    "            agg['investor'] = 'Investment trust'\n",
    "            agg = agg[['date','date_no','type','classify','qty','amt','investor']]\n",
    "            #display(agg)\n",
    "\n",
    "            for _ , data in agg.iterrows():\n",
    "                cur.execute(\"insert into ods.exponent_insti (date , date_no , type , topic , qty , amt , investor ) values (%s,%s,%s,%s,%s,%s,%s)\",data)\n",
    "            cur.execute('commit')\n",
    "            \n",
    "            print('【 Exponent Institutional 】{} of Investment Trust inserted. '.format(insert_D))\n",
    "            agg = ''\n",
    "        else:\n",
    "            pass\n",
    "        # 取得所有 type , topic\n",
    "        \n",
    "        all_type = pandas.read_sql(\"select distinct type from classify \",con = conn)\n",
    "        \n",
    "        \n",
    "        for i_type in range(len(all_type)):   \n",
    "        \n",
    "            TYPE = all_type.iloc[i_type][0]\n",
    "           \n",
    "            all_topic = pandas.read_sql(\"select distinct classify from classify where type = '{}'\".format(TYPE) , con = conn)\n",
    "            for i_topic in range(len(all_topic)):\n",
    "                \n",
    "                TOPIC = all_topic.iloc[i_topic][0]\n",
    "                \n",
    "                main = pandas.read_sql(\"select * from ods.exponent_insti \\\n",
    "                                         where type = '{0}' \\\n",
    "                                           and topic = '{1}' \\\n",
    "                                           and investor = 'Investment trust' \\\n",
    "                                           and date_no >= (select date_no from work_date where date = '{2}')-19 \\\n",
    "                                           and date_no <= (select date_no from work_date where date = '{2}') \\\n",
    "                                         order by date \".format(TYPE , TOPIC,insert_D) , con = conn)\n",
    "                \n",
    "                if len(main) == 20:\n",
    "                    main['qty20'].iloc[-1] = main['qty'].sum()\n",
    "                    main['amt20'].iloc[-1] = main['amt'].sum()\n",
    "                    DATE = main['date'].iloc[-1]\n",
    "                    #display(main.iloc[-1])\n",
    "                    \n",
    "                    cur.execute(\"update ods.exponent_insti set qty20 = '{0}' , amt20 = '{1}' \\\n",
    "                                  where date = '{2}' and type = '{3}' and topic = '{4}' and investor = 'Investment trust'\".format(main['qty20'].iloc[-1],main['amt20'].iloc[-1],DATE,TYPE,TOPIC))\n",
    "                    \n",
    "                    cur.execute(\"commit\")\n",
    "                else:\n",
    "                    pass\n",
    "        print('【 Exponent Institutional 】{0} qty20 & amt20 of Investment trust updated.'.format(insert_D))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 PythonOperator 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_IT_EXPONENT_ENERGY = PythonOperator(task_id = 'task_IT_EXPONENT_ENERGY' ,\n",
    "                                         python_callable = IT_EXPONENT_ENERGY ,\n",
    "                                         dag = dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REPORT_CCT20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 Function 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def REPORT_CCT20():\n",
    "    \n",
    "    insert_sql = \"insert into report.cct20_history \\\n",
    "              (date,no,name,price,cct5,cct20,fi_obs,it_obs,qty10,exponent,topic,tangled_num) \\\n",
    "              values (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\"\n",
    "    insert_c_sql = \"insert into report.cct20_history_c \\\n",
    "              (date,no,name,price,cct5,cct20,fi_obs,it_obs,qty10,exponent,topic,tangled_num) \\\n",
    "              values (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\"\n",
    "    \n",
    "    sma = pandas.read_sql(\"select date,no,close,sma5,sma10,sma20,sma60 from ods.analyze_avg where date_no = (select max(date_no) from work_date)\" , con = conn)\n",
    "    sma['糾結數量'] = 0\n",
    "    i = 0\n",
    "    for i in range(len(sma)):\n",
    "        if (sma['sma5'][i] <= sma['sma10'][i]*1.03 and sma['sma5'][i] >= sma['sma10'][i]*0.97) and \\\n",
    "        (sma['sma5'][i] <= sma['sma20'][i]*1.03 and sma['sma5'][i] >= sma['sma20'][i]*0.97) and \\\n",
    "        (sma['sma5'][i] <= sma['sma60'][i]*1.03 and sma['sma5'][i] >= sma['sma60'][i]*0.97):\n",
    "            sma['糾結數量'].iloc[i] = 4\n",
    "        elif (sma['sma5'][i] <= sma['sma10'][i]*1.03 and sma['sma5'][i] >= sma['sma10'][i]*0.97) and \\\n",
    "        (sma['sma5'][i] <= sma['sma20'][i]*1.03 and sma['sma5'][i] >= sma['sma20'][i]*0.97) :\n",
    "            sma['糾結數量'].iloc[i] = 3\n",
    "    sma = sma[['no','糾結數量']]\n",
    "    sma.columns = ['股票代號','糾結數量']\n",
    "    \n",
    "    a = pandas.read_sql(\"select main.* , fi.sum20 from (select *  from ods.cct20 c \\\n",
    "                  where c.date_no >= (select max(date_no) from work_date )-4 ) main join ods.analyze_fi_obs fi on main.date = fi.date and main.no = fi.no\\\n",
    "                  order by main.no , main.date_no\\\n",
    "                \" , con = conn)\n",
    "\n",
    "    a['bigger'] = ''\n",
    "    query_D = pandas.read_sql(\"select max(date) from work_date\" , con = conn).iloc[0,0]\n",
    "    \n",
    "    try:\n",
    "        os.mkdir('/home/buneo/Stock/CCT20/{:0>4}{:0>2}{:0>2}'.format(query_D.year , query_D.month , query_D.day))\n",
    "        pass\n",
    "    except FileExistsError:\n",
    "        pass \n",
    "    for i in range(1,len(a)):\n",
    "        if a['no'].iloc[i] == a['no'].iloc[i-1]:\n",
    "            if a['cct20'].iloc[i] > a['cct20'].iloc[i-1]:\n",
    "                a['bigger'].iloc[i] = 'Y'\n",
    "    b = a[a['bigger'] == 'Y'][['no','bigger']].groupby('no').count()\n",
    "    qqq = b[b['bigger'] == 4].index.tolist()\n",
    "    c =a[a['date'] == query_D]\n",
    "    mask = c['no'].isin(qqq)\n",
    "    clas = pandas.read_sql(\"select no , concat(classify,'/') topic from classify where type in ('topic','semi')\" , con = conn).groupby('no').sum('topic')\n",
    "    p = pandas.read_sql(\"select date , no , close , name from stock_daily where date = '{0}' union select date , no , close , name from otc_daily where date = '{0}' \".format(query_D) , con = conn)\n",
    "    it20 = pandas.read_sql(\"select no,date,sum20 as IT20 from ods.analyze_it_obs where date = '{}'\".format(query_D) , con = conn)\n",
    "    c = pandas.merge(c[['date','no','cct20','sum20']],clas,on='no',how = 'left')    \n",
    "    c = pandas.merge(c,p , on ='no')    \n",
    "    c = pandas.merge(c,it20 , on = 'no').iloc[:,[0,1,7,6,2,3,9,4]]\n",
    "    exp = pandas.read_sql(\"select no AS 股票代號 , classify AS 類股 from classify where type in ('otc','stock')\" , con = conn)\n",
    "    \n",
    "    c.columns = ['日期','股票代號','股票名稱','收盤價','20日集中度','20日外資買賣超','20日投信買賣超','題材']\n",
    "    c = pandas.merge(c,sma,on = '股票代號',how = 'left')\n",
    "    c = pandas.merge(c,exp,on = '股票代號' , how ='left')\n",
    "    c = c.iloc[:,[0,1,2,3,4,5,6,9,7,8]]\n",
    "    c5 = pandas.read_sql(\"select no , cct5 from ods.cct5 where date = '{}'\".format(query_D) , con = conn)\n",
    "    \n",
    "    qty10 = pandas.read_sql(\"select  no , cast(avg(quantity) as int) AS qty10 from stock_daily stock where stock.date in (select date from work_date where date_no >= (select max(date_no) from work_date ) -9 )  group by no union \\\n",
    "                            select  no , cast(avg(quantity) as int )AS qty10 from otc_daily stock where stock.date in (select date from work_date where date_no >= (select max(date_no) from work_date ) -9 )  group by no \" , con = conn)\n",
    "    cc10 = pandas.merge(left = c[c['股票代號'].isin(qqq)],right = qty10,left_on = '股票代號',right_on = 'no')\n",
    "    \n",
    "    ms1 = cc10['股票代號'].isin(qqq)\n",
    "    ms2 = cc10['20日集中度'] <= 0.5\n",
    "    ms3 = cc10['20日集中度'] >= -0.05\n",
    "    ms4 = cc10['qty10'] >= 500\n",
    "    report_cct10 = cc10[ms1 & ms2 & ms3 & ms4]\n",
    "    report_cct10 = pandas.merge(left = report_cct10 , right = c5 , left_on = '股票代號',right_on = 'no').iloc[:,[0,1,2,3,13,4,5,6,11,7,8,9]]\n",
    "    report_cct10.columns = ['日期','股票代號','股票名稱','收盤價','5日集中度','20日集中度','20日外資買賣超','20日投信買賣超','10日平均成交量','類股','題材','糾結數量']\n",
    "    cur.execute(\"delete from report.cct20_history where date = '{}'\".format(query_D))\n",
    "    cur.execute(\"commit\")\n",
    "    \n",
    "    for _ , data in report_cct10.iterrows():\n",
    "        cur.execute(insert_sql,data)\n",
    "    cur.execute(\"commit\")\n",
    "    \n",
    "    report_cct10.sort_values('20日集中度').to_csv('/home/buneo/Stock/CCT20/{0:0>4}{1:0>2}{2:0>2}/20日集中度_{1:0>2}{2:0>2}.csv'.format(query_D.year, query_D.month,query_D.day),index = False)\n",
    "    \n",
    "    qty5 = pandas.read_sql(\"select  no , avg(quantity) qty from stock_daily stock where stock.date in (select date from work_date where date_no >= (select max(date_no) from work_date ) -4 )  group by no union \\\n",
    "                        select  no , avg(quantity) qty from otc_daily stock where stock.date in (select date from work_date where date_no >= (select max(date_no) from work_date ) -4 )  group by no \" , con = conn)\n",
    "    cc5 = pandas.merge(c[c['股票代號'].isin(qqq)],qty5,left_on = '股票代號',right_on = 'no')\n",
    "    \n",
    "    mask_1 = cc5['qty']>=1500\n",
    "    mask_2 = cc5['收盤價'] >=5\n",
    "    mask_3 = cc5['收盤價'] <=150\n",
    "    mask_4 = cc5['20日集中度'] >= -0.1\n",
    "    report_cct5 = cc5[mask_1 & mask_2 & mask_3 & mask_4].iloc[:,[0,1,2,3,11,4,5,6,7,8,9]]\n",
    "    report_cct5 = pandas.merge(left = report_cct5 , right = c5 , left_on = '股票代號' , right_on = 'no').iloc[:,[0,1,2,3,12,5,6,7,4,8,9,10]]\n",
    "    \n",
    "    cur.execute(\"delete from report.cct20_history_c where date = '{}'\".format(query_D))\n",
    "    cur.execute(\"commit\")\n",
    "    \n",
    "    for _ , data in report_cct5.iterrows():\n",
    "        cur.execute(insert_c_sql,data)\n",
    "    cur.execute(\"commit\")\n",
    "    report_cct5.columns = ['日期','股票代號','股票名稱','收盤價','5日集中度','20日集中度','20日外資買賣超','20日投信買賣超','10日平均成交量','類股','題材','糾結數量']\n",
    "    report_cct5.sort_values('20日集中度').to_csv('/home/buneo/Stock/CCT20/{0:0>4}{1:0>2}{2:0>2}/20日集中度_{1:0>2}{2:0>2}_鄭大版.csv'.format(query_D.year, query_D.month,query_D.day),index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 PythonOperator 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_REPORT_CCT20 = PythonOperator(task_id = 'task_REPORT_CCT20' ,\n",
    "                                   python_callable = REPORT_CCT20 ,\n",
    "                                   dag = dag ,\n",
    "                                   trigger_rule = 'all_success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REPORT_GOODPRICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 Function 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def REPORT_GOODPRICE():\n",
    "    sma = pandas.read_sql(\"select date , no , close , sma10 , sma20 from ods.analyze_avg where date_no = (select max(date_no) from work_date) and (close < sma10 or close < sma20) \" , con = conn)\n",
    "    FI = pandas.read_sql(\"select date , no , sum10 , sum20 from ods.analyze_fi_obs where date_no = (select max(date_no) from work_date) and (sum10 > 300000 or sum20 > 300000)\" , con = conn)\n",
    "    name = pandas.read_sql(\"select date , no , name from stock_daily where date = (select max(date) from work_date)\\\n",
    "                     union \\\n",
    "                     select date , no , name from otc_daily where date = (select max(date) from work_date)\" , con = conn)\n",
    "    main = pandas.merge(sma,FI , on =['date','no'])\n",
    "    main = main[main['close'] > 0]\n",
    "    \n",
    "    main['v10'] = main.apply(lambda x : round((x['sma10'] - x['close']) / x['close'],4)*100  if (x['close'] < x['sma10']) and x['sum10'] > 0 else '' , axis = 1)\n",
    "    main['v20'] = main.apply(lambda x : round((x['sma20'] - x['close']) / x['close'],4)*100 if (x['close'] < x['sma20']) and x['sum20'] > 0 else '' , axis = 1)\n",
    "    main['v10'] = main.apply(lambda x : str(x['v10'])[:4] + '%' if x['v10'] != '' else '' , axis = 1 )\n",
    "    main['v20'] = main.apply(lambda x : str(x['v20'])[:4] + '%' if x['v20'] != '' else '', axis = 1 )\n",
    "    mask_1 = main['v10'] != ''\n",
    "    mask_2 = main['v20'] != ''\n",
    "    main = main[ mask_1 | mask_2 ]\n",
    "    main = main.sort_values(['sum20','sum10'] , ascending = False)\n",
    "    n_main = pandas.merge(main,name , on = ['date','no']).iloc[:,[0,1,9,2,3,4,5,6,7,8]]\n",
    "    query_D = n_main['date'][0]\n",
    "    n_main.to_csv('/home/buneo/Stock/GOOD_PRICE/GOOD_PRICE_{:0>4}{:0>2}{:0>2}.csv'.format(query_D.year , query_D.month , query_D.day),index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 PythonOperator 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_REPORT_GOODPRICE = PythonOperator(task_id = 'task_REPORT_GOODPRICE' ,\n",
    "                                   python_callable = REPORT_GOODPRICE ,\n",
    "                                   dag = dag ,\n",
    "                                   trigger_rule = 'all_success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REPORT_EXPONENT_ENERGY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 BashOperator 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_REPORT_EXPONENT_ENERGY = BashOperator(task_id = 'task_REPORT_EXPONENT_ENERGY',\n",
    "                                           bash_command = 'python /home/buneo/Stock/report_func/Report_exponent_energy.py' ,\n",
    "                                           dag = dag ,\n",
    "                                           trigger_rule = 'all_success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLACK_CCT20_MACD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 Function 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SLACK_CCT20_MACD():\n",
    "    \n",
    "    # 取最後交易日 , 讀取20日集中度報表\n",
    "    max_date = pandas.read_sql(\"select max(date) as date from work_date \" , con = conn)['date'].iloc[0]\n",
    "    \n",
    "    # 最後近20天交易日\n",
    "    date_list = tuple(pandas.read_sql(\"select * from work_date order by date desc limit 50\" , con = conn)['date_no'])\n",
    "    \n",
    "    try :\n",
    "        os.mkdir('/home/buneo/Stock/SLACK_REPORT/CCT20_SLACK/{}'.format(str(max_date)))\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    try :\n",
    "        os.mkdir('/home/buneo/Stock/SLACK_REPORT/CCT20_SLACK_MACD_GREEN/{}'.format(str(max_date)))\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    try :\n",
    "        os.mkdir('/home/buneo/Stock/SLACK_REPORT/CCT20_SLACK_C/{}'.format(str(max_date)))\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    try :\n",
    "        os.mkdir('/home/buneo/Stock/SLACK_REPORT/MACD_02/{}'.format(str(max_date)))\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    try :\n",
    "        os.mkdir('/home/buneo/Stock/SLACK_REPORT/MACD_24/{}'.format(str(max_date)))\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    # 20日集中度報表\n",
    "    cct20 = pandas.read_sql(\"select * from report.cct20_history where date ='{}'\".format(str(max_date)) , con = conn)\n",
    "    \n",
    "    # 20日集中度報表所有\"股票代號\"\n",
    "    no_list = cct20['no'].tolist()\n",
    "    \n",
    "    for no in no_list:\n",
    "        \n",
    "        # 讀目前股票代號取近20日MACD\n",
    "        no_macd = pandas.read_sql(\"select cast(date as varchar(10)) as date , no , macd_bar , macd , (mema-nema) as DIF from ods.macd where date_no in {0} and no = '{1}' order by date\".format(date_list , no) , con = conn)\n",
    "        no_macd = no_macd.reset_index()\n",
    "        no_macd['color'] = no_macd.apply(lambda x : 'red' if x['macd_bar'] > 0 else 'green' , axis = 1)\n",
    "        no_macd['xticks'] = no_macd.apply(lambda x : x['date'] if ((x['index']+1) % 5 == 0 or x['index'] == 0) else \"\" , axis = 1)   \n",
    "        max_bar = no_macd['macd_bar'].abs().max()\n",
    "        \n",
    "        # 將20日集中度數值設定為變數\n",
    "        tmp = cct20[cct20['no'] == no]\n",
    "        cct20_no = tmp['no'].iloc[0]\n",
    "        cct20_name = tmp['name'].iloc[0]\n",
    "        cct20_date = tmp['date'].iloc[0]\n",
    "        \n",
    "        fig , ax = plt.subplots(1,1,figsize = (12,5))\n",
    "        ax.set_title('{0} ({1}) '.format(cct20_no , cct20_name) , fontsize = 20)\n",
    "        ax.set_ylim(-max_bar , max_bar)\n",
    "        ax.grid(which = 'major')\n",
    "        ax.bar(x = no_macd['date'] , height = no_macd['macd_bar'] , align = 'center' , color = no_macd['color'] )\n",
    "        plt.xticks(no_macd['xticks'])\n",
    "        plt.savefig('/home/buneo/Stock/SLACK_REPORT/CCT20_SLACK/{0}/{2}{1}.png'.format(max_date ,cct20_name , cct20_no))\n",
    "        plt.close()\n",
    "        \n",
    "    # 20日集中度報表\n",
    "    cct20 = pandas.read_sql(\"select * from report.cct20_history_c where date ='{}'\".format(str(max_date)) , con = conn)\n",
    "    \n",
    "    # 20日集中度報表所有\"股票代號\"\n",
    "    no_list = cct20['no'].tolist()\n",
    "    \n",
    "    for no in no_list:\n",
    "        \n",
    "        # 讀目前股票代號取近20日MACD\n",
    "        no_macd = pandas.read_sql(\"select cast(date as varchar(10)) as date , no , macd_bar , macd , (mema-nema) as DIF from ods.macd where date_no in {0} and no = '{1}' order by date\".format(date_list , no) , con = conn)\n",
    "        no_macd = no_macd.reset_index()\n",
    "        no_macd['color'] = no_macd.apply(lambda x : 'red' if x['macd_bar'] > 0 else 'green' , axis = 1)\n",
    "        no_macd['xticks'] = no_macd.apply(lambda x : x['date'] if ((x['index']+1) % 5 == 0 or x['index'] == 0) else \"\" , axis = 1)   \n",
    "        max_bar = no_macd['macd_bar'].abs().max()\n",
    "        \n",
    "        # 將20日集中度數值設定為變數\n",
    "        tmp = cct20[cct20['no'] == no]\n",
    "        cct20_no = tmp['no'].iloc[0]\n",
    "        cct20_name = tmp['name'].iloc[0]\n",
    "        cct20_date = tmp['date'].iloc[0]\n",
    "        \n",
    "        fig , ax = plt.subplots(1,1,figsize = (12,5))\n",
    "        ax.set_title('{0} ({1})'.format(cct20_no , cct20_name) , fontsize = 20)\n",
    "        ax.set_ylim(-max_bar , max_bar)\n",
    "        ax.grid(which = 'major')\n",
    "        ax.bar(x = no_macd['date'] , height = no_macd['macd_bar'] , align = 'center' , color = no_macd['color'] )\n",
    "        plt.xticks(no_macd['xticks'])\n",
    "        plt.savefig('/home/buneo/Stock/SLACK_REPORT/CCT20_SLACK_C/{0}/{2}{1}.png'.format(max_date ,cct20_name, cct20_no))\n",
    "        plt.close()\n",
    "        \n",
    "    \n",
    "    cct20 = pandas.read_sql(\"select * from ods.cct20 where date ='{}'\".format(str(max_date)) , con = conn)\n",
    "    \n",
    "    macd = pandas.read_sql(\"select no , date_no from ods.macd where date = '{0}' and macd_bar < 0\".format(max_date) , con = conn)\n",
    "    \n",
    "    green_macd_no = macd['no'].tolist()\n",
    "    date_no = macd['date_no'].unique()[0]-3\n",
    "    \n",
    "    main = pandas.read_sql(\"select * from ods.macd where date_no >= '{0}' and no in {1} order by no , date\".format(date_no,tuple(green_macd_no)), con = conn)\n",
    "    main['bigger'] = ''\n",
    "    \n",
    "    for i in range(1,len(main)):#range(len(main)):\n",
    "        if main['no'][i] == main['no'][i-1] :\n",
    "            if main['macd_bar'][i] >= main['macd_bar'][i-1]:\n",
    "                main['bigger'][i] = main['bigger'][i-1]+'Y'\n",
    "        else :\n",
    "            main['bigger'][i] = ''\n",
    "    \n",
    "    rise_no = main[main['bigger'] == 'YYY'][['no']]\n",
    "    no_5d = pandas.read_sql(\"select no , count(*) from (\\\n",
    "                                 select no from stock_daily s join work_date d on s.date = d.date and d.date_no >= {0}\\\n",
    "                                 union all \\\n",
    "                                 select no from   otc_daily o join work_date d on o.date = d.date and d.date_no >= {0}) a\\\n",
    "                                 group by no having count(*) = 5\".format(date_no) , con = conn)['no'].tolist()\n",
    "    vol_1000 = pandas.read_sql(\"select no , avg(qty) from (select no , quantity qty from stock_daily s join work_date d on s.date = d.date and d.date_no >= {0} \\\n",
    "                                                           union all \\\n",
    "                                                           select no , quantity qty from   otc_daily o join work_date d on o.date = d.date and d.date_no >= {0} ) a \\\n",
    "                                group by no having avg(qty) >= 1000\".format(date_no) , con = conn)\n",
    "    image_no = pandas.merge(left = rise_no , right = vol_1000 , on = 'no')\n",
    "    \n",
    "    price_no = pandas.read_sql(\"select date , no from ods.analyze_avg where date_no = '{0}' and close > sma60\".format(date_no+3) , con = conn)[['no']]\n",
    "    \n",
    "    image_no = pandas.merge(left = image_no , right = price_no , on = 'no')['no'].tolist()\n",
    "    \n",
    "    \n",
    "    for no in image_no:\n",
    "    \n",
    "        # 讀目前股票代號取近20日MACD\n",
    "        no_macd = pandas.read_sql(\"select cast(date as varchar(10)) as date , no , macd_bar , macd , (mema-nema) as DIF from ods.macd where date_no in {0} and no = '{1}' order by date\".format(date_list , no) , con = conn)\n",
    "        no_macd = no_macd.reset_index()\n",
    "        no_macd['color'] = no_macd.apply(lambda x : 'red' if x['macd_bar'] > 0 else 'green' , axis = 1)\n",
    "        no_macd['xticks'] = no_macd.apply(lambda x : x['date'] if ((x['index']+1) % 5 == 0 or x['index'] == 0) else \"\" , axis = 1)   \n",
    "        max_bar = no_macd['macd_bar'].abs().max()\n",
    "        \n",
    "        # 將20日集中度數值設定為變數\n",
    "        tmp = cct20[cct20['no'] == no]\n",
    "        cct20_no = tmp['no'].iloc[0]\n",
    "        cct20_name = pandas.read_sql(\"select no , name from stock_daily where date = '{0}' and no = '{1}' union select no , name from otc_daily where date = '{0}' and no = '{1}'\".format(max_date , no) , con = conn)['name'].iloc[0]\n",
    "        cct20_date = tmp['date'].iloc[0]\n",
    "        \n",
    "        fig , ax = plt.subplots(1,1,figsize = (12,5))\n",
    "        ax.set_title('{0} ({1})'.format(cct20_no , cct20_name) , fontsize = 20)\n",
    "        ax.set_ylim(-max_bar , max_bar)\n",
    "        ax.grid(which = 'major')\n",
    "        ax.bar(x = no_macd['date'] , height = no_macd['macd_bar'] , align = 'center' , color = no_macd['color'] )\n",
    "        plt.xticks(no_macd['xticks'])\n",
    "        plt.savefig('/home/buneo/Stock/SLACK_REPORT/CCT20_SLACK_MACD_GREEN/{0}/{2}{1}.png'.format(max_date ,cct20_name , cct20_no))\n",
    "        plt.close()\n",
    "        \n",
    "    \n",
    "    vol_600 = pandas.read_sql(\"select no , avg(qty) from (select no , quantity qty from stock_daily s join work_date d on s.date = d.date and d.date_no >= {0} \\\n",
    "                                                           union all \\\n",
    "                                                           select no , quantity qty from   otc_daily o join work_date d on o.date = d.date and d.date_no >= {0} ) a \\\n",
    "                                group by no having avg(qty) >= 600\".format(date_no) , con = conn)\n",
    "    price_no = pandas.read_sql(\"select date , no from ods.analyze_avg where date = '{0}' and close > 15 and close < 300 \".format(max_date) , con = conn)[['no']]\n",
    "    \n",
    "    macd_02 = pandas.read_sql(\"select no , date_no from ods.macd where date = '{0}' and macd_bar <= 0 and macd_bar >= -2 \".format(max_date) , con = conn)\n",
    "    macd_02 = pandas.merge(macd_02 , vol_600 , on = 'no')\n",
    "    macd_02 = pandas.merge(macd_02 , price_no , on = 'no')['no'].tolist()\n",
    "    \n",
    "    for no in macd_02:\n",
    "    \n",
    "        # 讀目前股票代號取近20日MACD\n",
    "        no_macd = pandas.read_sql(\"select cast(date as varchar(10)) as date , no , macd_bar , macd , (mema-nema) as DIF from ods.macd where date_no in {0} and no = '{1}' order by date\".format(date_list , no) , con = conn)\n",
    "        no_macd = no_macd.reset_index()\n",
    "        no_macd['color'] = no_macd.apply(lambda x : 'red' if x['macd_bar'] > 0 else 'green' , axis = 1)\n",
    "        no_macd['xticks'] = no_macd.apply(lambda x : x['date'] if ((x['index']+1) % 5 == 0 or x['index'] == 0) else \"\" , axis = 1)   \n",
    "        max_bar = no_macd['macd_bar'].abs().max()\n",
    "        \n",
    "        # 將20日集中度數值設定為變數\n",
    "        tmp = cct20[cct20['no'] == no]\n",
    "        cct20_no = tmp['no'].iloc[0]\n",
    "        cct20_name = pandas.read_sql(\"select no , name from stock_daily where date = '{0}' and no = '{1}' union select no , name from otc_daily where date = '{0}' and no = '{1}'\".format(max_date , no) , con = conn)['name'].iloc[0]\n",
    "        cct20_date = tmp['date'].iloc[0]\n",
    "        \n",
    "        fig , ax = plt.subplots(1,1,figsize = (12,5))\n",
    "        ax.set_title('{0} ({1})'.format(cct20_no , cct20_name) , fontsize = 20)\n",
    "        ax.set_ylim(-max_bar , max_bar)\n",
    "        ax.grid(which = 'major')\n",
    "        ax.bar(x = no_macd['date'] , height = no_macd['macd_bar'] , align = 'center' , color = no_macd['color'] )\n",
    "        plt.xticks(no_macd['xticks'])\n",
    "        plt.savefig('/home/buneo/Stock/SLACK_REPORT/MACD_02/{0}/{2}{1}.png'.format(max_date ,cct20_name , cct20_no))\n",
    "        plt.close()\n",
    "        \n",
    "    \n",
    "    macd_24 = pandas.read_sql(\"select no , date_no from ods.macd where date = '{0}' and macd_bar <= -2 and macd_bar >= -4 \".format(max_date) , con = conn)\n",
    "    macd_24 = pandas.merge(macd_24 , vol_600 , on = 'no')\n",
    "    macd_24 = pandas.merge(macd_24 , price_no , on = 'no')['no'].tolist()\n",
    "    \n",
    "    for no in macd_24:\n",
    "    \n",
    "        # 讀目前股票代號取近20日MACD\n",
    "        no_macd = pandas.read_sql(\"select cast(date as varchar(10)) as date , no , macd_bar , macd , (mema-nema) as DIF from ods.macd where date_no in {0} and no = '{1}' order by date\".format(date_list , no) , con = conn)\n",
    "        no_macd = no_macd.reset_index()\n",
    "        no_macd['color'] = no_macd.apply(lambda x : 'red' if x['macd_bar'] > 0 else 'green' , axis = 1)\n",
    "        no_macd['xticks'] = no_macd.apply(lambda x : x['date'] if ((x['index']+1) % 5 == 0 or x['index'] == 0) else \"\" , axis = 1)   \n",
    "        max_bar = no_macd['macd_bar'].abs().max()\n",
    "        \n",
    "        # 將20日集中度數值設定為變數\n",
    "        tmp = cct20[cct20['no'] == no]\n",
    "        cct20_no = tmp['no'].iloc[0]\n",
    "        cct20_name = pandas.read_sql(\"select no , name from stock_daily where date = '{0}' and no = '{1}' union select no , name from otc_daily where date = '{0}' and no = '{1}'\".format(max_date , no) , con = conn)['name'].iloc[0]\n",
    "        cct20_date = tmp['date'].iloc[0]\n",
    "        \n",
    "        fig , ax = plt.subplots(1,1,figsize = (12,5))\n",
    "        ax.set_title('{0} ({1})'.format(cct20_no , cct20_name) , fontsize = 20)\n",
    "        ax.set_ylim(-max_bar , max_bar)\n",
    "        ax.grid(which = 'major')\n",
    "        ax.bar(x = no_macd['date'] , height = no_macd['macd_bar'] , align = 'center' , color = no_macd['color'] )\n",
    "        plt.xticks(no_macd['xticks'])\n",
    "        plt.savefig('/home/buneo/Stock/SLACK_REPORT/MACD_24/{0}/{2}{1}.png'.format(max_date ,cct20_name , cct20_no))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 PythonOperator 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_SLACK_CCT20_MACD = PythonOperator(task_id = 'task_SLACK_CCT20_MACD',\n",
    "                                       python_callable = SLACK_CCT20_MACD , \n",
    "                                       dag = dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLACK_MESSAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 Function 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SLACK_MESSAGE():\n",
    "    \n",
    "    FILE_LIST = {'IMAGE_CCT20_NORMAL': {'LOCAL_FOLDER' : 'CCT20_SLACK' , 'CH':'cct20-一般版' , 'TABLE':'cct20_history' , 'FILE_NAME':'/20日集中度_{0}.csv'\n",
    "                                    ,'CH_ID' : 'C02DF5Y0SQ3'\n",
    "                                    ,'SLACK_URL':'https://hooks.slack.com/services/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'},\n",
    "             'IMAGE_CCT20_JHENG': {'LOCAL_FOLDER' : 'CCT20_SLACK_C' , 'CH':'cct20-鄭大版', 'TABLE':'cct20_history_c' , 'FILE_NAME':'/20日集中度_{0}_鄭大版.csv'\n",
    "                                    ,'CH_ID' : 'C02DF632PL2'\n",
    "                                    ,'SLACK_URL':'https://hooks.slack.com/services/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'},\n",
    "             'IMAGE_CCT20_MACD':{'LOCAL_FOLDER' : 'CCT20_SLACK_MACD_GREEN' , 'CH' : 'cct20-macd' , 'TABLE' : ''\n",
    "                                    ,'CH_ID' : 'C02DC5L3U1K'\n",
    "                                    ,'SLAKC_URL':'https://hooks.slack.com/services/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'}}\n",
    "\n",
    "    SLACK_TOKEN = Variable.get(\"SLACK_TOKEN\")\n",
    "    DICT_PARAM = [{'name':'IMAGE_CCT20_NORMAL'},{'name':'IMAGE_CCT20_JHENG'},{'name':'IMAGE_CCT20_MACD'}]\n",
    "    ADVICE_LIST= '<@aa0952828017><@s912763><@buneostock>'   \n",
    "    lastest_date = str(pandas.read_sql(\"select date from work_date order by date desc limit 1\" , con = conn)['date'].iloc[0])\n",
    "    \n",
    "    client = slack_sdk.WebClient(token = SLACK_TOKEN)\n",
    "    \n",
    "    for i in range(len(FILE_LIST)):\n",
    "        \n",
    "        local_path = '/home/buneo/Stock/SLACK_REPORT/{0}/{1}'.format(FILE_LIST[DICT_PARAM[i]['name']]['LOCAL_FOLDER'] , lastest_date)\n",
    "        items = sorted(os.listdir(local_path))\n",
    "        items = items\n",
    "        #items = service.files().list(q = \"'\" + DICT_PARAM[i]['sub_id'] + \"' in parents and trashed = False \").execute()['files'][:1]\n",
    "        channel_name = FILE_LIST[DICT_PARAM[i]['name']]['CH']\n",
    "        table_name = FILE_LIST[DICT_PARAM[i]['name']]['TABLE']\n",
    "        excel_path = '/home/buneo/Stock/CCT20/{}/'.format(lastest_date.replace('-',''))\n",
    "        \n",
    "        if table_name != '' :\n",
    "            \n",
    "            table = pandas.read_sql(\"select * from report.{0} where date = '{1}'\".format(table_name , lastest_date) , con = conn)        \n",
    "            for idx , item in enumerate(items) :\n",
    "                \n",
    "                file_path = '/home/buneo/Stock/SLACK_REPORT/{0}/{1}/{2}'.format(FILE_LIST[DICT_PARAM[i]['name']]['LOCAL_FOLDER'],lastest_date , item)\n",
    "                data = table[table['no'] == item[:4]].iloc[0]\n",
    "                \n",
    "                try:\n",
    "                    response = client.files_upload(file = file_path,\n",
    "                                               initial_comment = \"{12}/{13}\\r\\n日期 : {0}\\r\\nNO : <https://www.wantgoo.com/stock/{1}/technical-chart|{1}>\\r\\n名稱 : {2}\\r\\n收盤價 : {3}\\r\\n\\\n",
    "5日集中度 : {4}\\r\\n20日集中度 : {5}\\r\\n20日外資籌碼 : {6}股\\r\\n20日投信籌碼 : {7}股\\r\\n10日平均交易量 : {8}張\\r\\n類股 : {9}\\r\\n\\\n",
    "題材 : {10}\\r\\n均線糾結數量 : {11}\".format(data['date'],data['no'],data['name'],data['price'],data['cct5'],\n",
    "                                          data['cct20'],data['fi_obs'],data['it_obs'],data['qty10'],data['exponent'],\n",
    "                                          data['topic'][:-1],data['tangled_num'],idx+1,len(items)),\n",
    "                                               channels = channel_name )\n",
    "                except SlackApiError as e:\n",
    "                    assert e.response[\"ok\"] is False\n",
    "                    assert e.response[\"error\"]  # str like 'invalid_auth', 'channel_not_found'\n",
    "                    print(f\"Got an error: {e.response['error']}\")\n",
    "                    \n",
    "            try:\n",
    "                response = client.files_upload(initial_comment = \"{2} {0}資料已經上傳了，總共{1}筆\".format(lastest_date , len(items) , ADVICE_LIST),\n",
    "                                               file = excel_path + FILE_LIST[DICT_PARAM[i]['name']]['FILE_NAME'].format(lastest_date.replace('-','')[4:]),\n",
    "                                               channels = channel_name )\n",
    "            except SlackApiError as e:\n",
    "                assert e.response[\"ok\"] is False\n",
    "                assert e.response[\"error\"]  # str like 'invalid_auth', 'channel_not_found'\n",
    "                print(f\"Got an error: {e.response['error']}\")\n",
    "            \n",
    "            \n",
    "        else  :\n",
    "            table = None\n",
    "            \n",
    "            for item in items:\n",
    "                file_path = '/home/buneo/Stock/SLACK_REPORT/{0}/{1}/{2}'.format(FILE_LIST[DICT_PARAM[i]['name']]['LOCAL_FOLDER'],lastest_date , item)\n",
    "\n",
    "                try:\n",
    "                    response = client.files_upload(file = file_path,                                                \n",
    "                                                   channels = channel_name )\n",
    "                except SlackApiError  as e :\n",
    "                    assert e.response[\"ok\"] is False\n",
    "                    assert e.response[\"error\"]  # str like 'invalid_auth', 'channel_not_found'\n",
    "                    print(f\"Got an error: {e.response['error']}\")\n",
    "                    \n",
    "            try:\n",
    "                response = client.chat_postMessage(text = \"{2} {0}資料已經上傳了，總共{1}筆\".format(lastest_date , len(items) , ADVICE_LIST),\n",
    "                                                   channel = FILE_LIST[DICT_PARAM[i]['name']]['CH_ID'])\n",
    "            except SlackApiError as e:\n",
    "                assert e.response[\"ok\"] is False\n",
    "                assert e.response[\"error\"]  # str like 'invalid_auth', 'channel_not_found'\n",
    "                print(f\"Got an error: {e.response['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 PythonOperator 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_SLACK_MESSAGE = PythonOperator(task_id = 'task_SLACK_MESSAGE',\n",
    "                                   python_callable = SLACK_MESSAGE,\n",
    "                                   dag =dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UPLOAD_GOOGLE_DRIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【 BashOperator 】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_UPLOAD_GOOGLE_DRIVE = BashOperator(task_id = 'task_UPLOAD_GOOGLE_DRIVE',\n",
    "                                           bash_command = 'python /home/buneo/Stock/all_func/GOOGLE_DRIVES.py' ,\n",
    "                                           dag = dag ,\n",
    "                                           trigger_rule = 'all_success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'task_CHECK_WORK_DATE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-6504241811db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtask_CHECK_WORK_DATE\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtask_INSTI_INVESTOR_SUMMARIZE\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtask_STOCK_SMA\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtask_OTC_SMA\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtask_FI_OBS_SUM\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtask_IT_OBS_SUM\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtask_DL_OBS_SUM\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtask_MACD\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtask_BIAS\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtask_KD\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtask_CCT20\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtask_FI_EXPONENT_ENERGY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtask_CHECK_WORK_DATE\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mtask_NOT_TRADED_DATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtask_CHECK_WORK_DATE\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mtask_ERROR_TODAY_DATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtask_CCT20\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mtask_CCT5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtask_FI_EXPONENT_ENERGY\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mtask_IT_EXPONENT_ENERGY\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mtask_REPORT_EXPONENT_ENERGY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'task_CHECK_WORK_DATE' is not defined"
     ]
    }
   ],
   "source": [
    "task_CHECK_WORK_DATE >> [task_INSTI_INVESTOR_SUMMARIZE , task_STOCK_SMA , task_OTC_SMA , task_FI_OBS_SUM , task_IT_OBS_SUM , task_DL_OBS_SUM , task_MACD , task_BIAS , task_KD , task_CCT20 , task_FI_EXPONENT_ENERGY]\n",
    "task_CHECK_WORK_DATE >> task_NOT_TRADED_DATE\n",
    "task_CHECK_WORK_DATE >> task_ERROR_TODAY_DATA\n",
    "task_CCT20 >> task_CCT5\n",
    "task_FI_EXPONENT_ENERGY >> task_IT_EXPONENT_ENERGY >> task_REPORT_EXPONENT_ENERGY\n",
    "[task_CCT5 , task_STOCK_SMA , task_OTC_SMA , task_FI_OBS_SUM , task_IT_OBS_SUM ] >> task_REPORT_CCT20 >> task_SLACK_CCT20_MACD >> task_SLACK_MESSAGE >> task_UPLOAD_GOOGLE_DRIVE\n",
    "[task_STOCK_SMA , task_OTC_SMA , task_FI_OBS_SUM ] >> task_REPORT_GOODPRICE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "338.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
